#-*-coding: utf-8 -*-import sysimport codecs import nltkfrom nltk import bigramsimport mathdef EstraiTestoTokenizzato(frasi) :        tokensTOT=[] #lista in cui inserisco i tokens        for frase in frasi:                tokens=nltk.word_tokenize(frase) #divido la frase in token                tokensTOT=tokensTOT+tokens        return tokensTOT #restiuisco i tokensdef AnnotazioneLinguistica(frasi) :        tokensTOT=[] #lista in cui inserisco i tokens        tokensPOStot=[] #lista in cui inserisco l'analisi POS dei tokens        for frase in frasi :                tokens=nltk.word_tokenize(frase) #divido le frasi in token                tokensPOS=nltk.pos_tag(tokens) #analisi POS dei tokens                tokensTOT=tokensTOT+tokens                tokensPOStot=tokensPOStot+tokensPOS        return tokensPOStot #restituisco l'analisi POS dei tokendef LunghezzaMediaFrasi(frasi, numerotokens):        lunghezzafrasi = len(frasi) #calcolo la lunghezza delle frasi        Media = (numerotokens*1.0/lunghezzafrasi*1.0) #per calcolare la media, divido il numero dei tokens per la lunghezza delle frasi        return Media #restituisco la mediadef LunghezzaMediaToken(tokens, numerotokens):        caratteri = 0.0 #inizializzo la variabile caratteri        for tok in tokens:                 caratteri = caratteri + len(tok) #conto i tokens e li sommo al contatore                 Media = (caratteri*1.0/numerotokens*1.0) #per ottenere la media, divido i caratteri per il numero dei token        return Media #restituisco la mediadef GrandezzaVocabolario(frasi) :        corpus = [] #lista in cui inserisco i token del corpus        for frase in frasi:                tokens = nltk.word_tokenize(frase) #divido le frasi in token                corpus = corpus + tokens #aggiungo i tokens alla lista        PTipo = set(corpus) #set serve per togliere gli elementi uguali (restituisco le parole tipo)        Vocabolario = len(PTipo) #conto le parole tipo        return Vocabolariodef ContoHapax(tokens, numero):        Lista = tokens[0:numero] #lista in cui inserisco i token che vanno da 0 a 1000, da         conta = 0 #contatore        for tok in Lista:                if Lista.count(tok) == 1: #controllo che la frequenza del token sia 1                        conta = conta + 1 #aggiorno il contatore        return contadef SostantiviVerbi(TestoAnalizzatoPOS):        Lista=[] #Lista in cui inserisco i POS        for (token, pos) in TestoAnalizzatoPOS:                Lista.append(pos)         frequenza=nltk.FreqDist(Lista) #calcolo la frequenza dei POS        Sostantivi = (frequenza['NN']+frequenza['NNS']+frequenza['NNP']+frequenza['NNPS']) #sommo la frequenza dei sostantivi        Verbi = (frequenza['VB']+frequenza['VBD']+frequenza['VBG']+frequenza['VBN']+frequenza['VBP']+frequenza['VPZ']) #sommo la frequenza dei verbi        return (Sostantivi*1.0/Verbi*1.0) #restituisco il rappoto tra sostantivi e verbi   def POS (TestoAnalizzatoPOS):    listaPOS = [] #lista in cui inserisco i POS    for (tok, pos) in TestoAnalizzatoPOS:        listaPOS.append(pos) #aggiungo alla lista solo i POS e non i token    return listaPOSdef DieciPOS(listaPOS):    frequenza = nltk.FreqDist(listaPOS) #calcolo la frequenza dei POS    Dieci = frequenza.most_common(10)     return Dieci #restituisco i 10 POS più frequentidef ProbCond(listaPOS, diversi, bigrammi):    Pcond = {} #dizionario in cui inserisco il risultato      for bigramma in diversi:        frequenzaB = bigrammi.count(bigramma) #conto quante volte compare il bigramma        frequenzaA = listaPOS.count(bigramma[0]) #conto quante volte compare il primo elemento del bigramma nella lista POS        p_cond = frequenzaB*1.0/frequenzaA*1.0 #calcolo la probabilità condizionata         Pcond[bigramma] = p_cond #inserisco la probabilità condizionata nel dizionario        PcondOrdina = sorted(Pcond.items(), key = lambda x: x[1], reverse = True) #ordino il dizionario         DieciPcond = PcondOrdina[0:10] #prendo i primi 10 risultati     return DieciPconddef LocalMutual(listaPOS, ContaPOS, bigrammi):    LMI = {} #dizionario in cui inserisco il risultato    for bigramma in bigrammi:        FOsservata = bigrammi.count(bigramma) #frequenza osservata (ovvero il numero di volte che il bigramma compare nel testo)        Freq1 = listaPOS.count(bigramma[0]) #conto quante volte compare il primo elemento del bigramma nella lista POS        Freq2 = listaPOS.count(bigramma[1]) #conto quante volte compare il secondo elemento del bigramma nella lista POS        FAttesa = ((Freq1*1.0)*(Freq2*1.0)/(ContaPOS*1.0)) #calcola frequenza attesa (frequenza del primo elemento del bigramma*frequenza del secondo elemento/numero di POS)           MI = math.log((FOsservata*1.0/FAttesa*1.0),2) #calcolo la Mutual Information: logaritmo (in base 2) del rapporto tra frequenza osservata e frequenza attesa        LocalMutual = (FOsservata*1.0)*MI #moltiplico la MI per la frequenza osservata e ottengo la LMI        LMI[bigramma] = LocalMutual #inserisco la local mutual information nel dizionario        LMIOrdina = sorted(LMI.items(), key = lambda x: x[1], reverse = True) #ordino il dizionario        DieciLMI = LMIOrdina[0:10] #prendo i primi 10 risultati     return DieciLMI#definisco la main def main(file1,file2) :        fileInput1 = codecs.open(file1, "r", "utf-8") #apro i file con codecs.open        fileInput2 = codecs.open(file2, "r", "utf-8")        raw1 = fileInput1.read() #leggo i files con read        raw2 = fileInput2.read()        sent_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle') #tokenizzatore NLTK        frasi1 = sent_tokenizer.tokenize(raw1) #divido i corpus in frasi        frasi2 = sent_tokenizer.tokenize(raw2)        TestoAnalizzatoPOS1 = AnnotazioneLinguistica(frasi1) #annoto le frasi linguisticamente        TestoAnalizzatoPOS2 = AnnotazioneLinguistica(frasi2)        TestoTokenizzato1 = EstraiTestoTokenizzato(frasi1) #divido il testo in token        TestoTokenizzato2 = EstraiTestoTokenizzato(frasi2)        POS1 = POS(TestoAnalizzatoPOS1) #funzione che estrae i POS        POS2 = POS(TestoAnalizzatoPOS2)        numerofrasi1 = len(frasi1) #conto le frasi del testo         numerofrasi2 = len(frasi2)        print ("Il file", file1, "contiene", numerofrasi1, "frasi")        print ("Il file", file2, "contiene", numerofrasi2, "frasi")        if numerofrasi1>numerofrasi2 : #confronto i due testi sulla base del numero delle frasi                print (file1, "contiene più frasi di", file2)        elif numerofrasi1<numerofrasi2 :                print (file2, "contiene più frasi di", file1)        else:                print ("i due file contengono lo stesso numero di frasi")        numerotoken1 = len(TestoTokenizzato1) #conto i token del testo         numerotoken2 = len(TestoTokenizzato2)        print ("\nIl file", file1, "è lungo", numerotoken1, "token")        print ("Il file", file2, "è lungo", numerotoken2, "token")        if numerotoken1>numerotoken2 : #confronto i testi sulla base del numero dei token                print (file1, "è più lungo di", file2)        elif numerotoken1<numerotoken2 :                print (file2, "è più lungo di", file1)        else:                print ("i due file hanno la stessa lunghezza")        mediafrasi1 = LunghezzaMediaFrasi(frasi1, numerotoken1) #funzione che calcola la lunghezza media delle frasi        mediafrasi2 = LunghezzaMediaFrasi(frasi2, numerotoken2)        print ("\nLa lunghezza media delle frasi del", file1, "è", mediafrasi1)        print ("La lunghezza media delle frasi del", file2, "è", mediafrasi2)        if mediafrasi1>mediafrasi2 : #confronto i due file in base alla lunghezza media delle frasi                print ("La lunghezza media delle frasi del", file1, "è maggiore di quelle del", file2)        elif mediafrasi1<mediafrasi2 :                print ("La lunghezza media delle frasi del", file1, "è minore di quelle del", file2)        else:                print ("La lunghezza media delle frasi dei due file è la stessa")            media1 = LunghezzaMediaToken(TestoTokenizzato1, numerotoken1) #funzione che calcola la lunghezza media dei tokens        media2 = LunghezzaMediaToken(TestoTokenizzato2, numerotoken2)        print("\nLa lunghezza media dei token del", file1, "è", media1)        print("La lunghezza media dei token del", file2, "è", media2)        if media1>media2 : #confronto i due file in base alla lunghezza media dei token                print("La lunghezza media dei token del", file1, "è maggiore di quella del", file2)        elif media1<media2 :                print("La lunghezza media dei token del", file1, "è minore di quella del", file2)        else:                print("La lunghezza media dei token dei due file è la stessa")                vocabolario1 = GrandezzaVocabolario(frasi1) #funzione che calcola la grandezza del vocabolario         vocabolario2 = GrandezzaVocabolario(frasi2)        print("\nLa grandezza del vocabolario del", file1, "è", vocabolario1)        print("La grandezza del vocabolario del", file2, "è", vocabolario2)        if vocabolario1>vocabolario2 : #confronto i due file in base alla grandezza del vocabolario                 print("La grandezza del vocabolario del", file1, "è maggiore di quella del", file2)        elif vocabolario1<vocabolario2:                print("La grandezza del vocabolario del", file1, "è minore di quella del", file2)        else:                print("La grandezza dei due vocabolari è la stessa")                Rapporto1 = (SostantiviVerbi(TestoAnalizzatoPOS1)) #funzione che calcola il rapporto tra sostantivi e verbi        Rapporto2 = (SostantiviVerbi(TestoAnalizzatoPOS2))        print("\nIl rapporto tra Sostantivi e verbi del", file1, "è", Rapporto1)        print("Il rapporto tra sostantivi e verbi del", file2, "è", Rapporto2)        if Rapporto1>Rapporto2: #confronto i due file in base al rapporto tra sostantivi e verbi                print("Il rapporto tra sostantivi e verbi nel", file1, "è maggiore rispetto a quello del", file2)        elif Rapporto1<Rapporto2:                print("Il rapporto tra sostantivi e verbi nel", file1, "è minore rispetto a quello del", file2)        else:                print("Il rapporto tra sostantivi e verbi è lo stesso nei due file")                #conto gli hapax                   Hapax11000 = ContoHapax(TestoTokenizzato1, 1000)        Hapax21000 = ContoHapax(TestoTokenizzato2, 1000)        print("\nHapax 1000:",Hapax11000, "del", file1)        print("Hapax 1000:",Hapax21000, "del", file2)        Hapax12000 = ContoHapax(TestoTokenizzato1, 2000)        Hapax22000 = ContoHapax(TestoTokenizzato2, 2000)        print("Hapax 2000:",Hapax12000, "del", file1)        print("Hapax 2000:",Hapax22000, "del", file2)        Hapax13000 = ContoHapax(TestoTokenizzato1, 3000)        Hapax23000 = ContoHapax(TestoTokenizzato2, 3000)        print("Hapax 3000:",Hapax13000, "del", file1)        print("Hapax 3000:",Hapax23000, "del", file2)        Hapax14000 = ContoHapax(TestoTokenizzato1, 4000)        Hapax24000 = ContoHapax(TestoTokenizzato2, 4000)        print("Hapax 4000:",Hapax14000, "del", file1)        print("Hapax 4000:",Hapax24000, "del", file2)        Hapax15000 = ContoHapax(TestoTokenizzato1, 5000)        Hapax25000 = ContoHapax(TestoTokenizzato2, 5000)        print("Hapax 5000:",Hapax15000, "del", file1)        print("Hapax 5000:",Hapax25000, "del", file2)        Hapax16000 = ContoHapax(TestoTokenizzato1, 6000)        Hapax26000 = ContoHapax(TestoTokenizzato2, 6000)        print("Hapax 6000:",Hapax16000, "del", file1)        print("Hapax 6000:",Hapax26000, "del", file2)        Hapax17000 = ContoHapax(TestoTokenizzato1, 7000)        Hapax27000 = ContoHapax(TestoTokenizzato2, 7000)        print("Hapax 7000:",Hapax17000, "del", file1)        print("Hapax 7000:",Hapax27000, "del", file2)        Hapax18000 = ContoHapax(TestoTokenizzato1, 8000)        Hapax28000 = ContoHapax(TestoTokenizzato2, 8000)        print("Hapax 8000:",Hapax14000, "del", file1)        print("Hapax 8000:",Hapax24000, "del", file2)        Hapax19000 = ContoHapax(TestoTokenizzato1, 9000)        Hapax29000 = ContoHapax(TestoTokenizzato2, 9000)        print("Hapax 9000:",Hapax19000, "del", file1)        print("Hapax 9000:",Hapax29000, "del", file2)        Hapax110000 = ContoHapax(TestoTokenizzato1, 10000)        Hapax210000 = ContoHapax(TestoTokenizzato2, 10000)        print("Hapax 10000:",Hapax110000, "del", file1)        print("Hapax 10000:",Hapax210000, "del", file2)                        DieciPartOfSpeech1 = DieciPOS(POS1) #estraggo i 10 POS più frequenti        DieciPartOfSpeech2 = DieciPOS(POS2)                print("\nI dieci POS più frequenti del", file1, "sono", DieciPartOfSpeech1) #stampo i 10 POS più frequenti         print("\nI dieci POS più frequenti del", file2, "sono", DieciPartOfSpeech2)                        bigrammiPOS1 = list(bigrams(POS1)) #funzione che calcola i bigrammi di POS        bigrammiPOS2 = list(bigrams(POS2))        diversi1 = set(bigrammiPOS1) #estraggo i bigrammi di POS senza ripetizioni         diversi2 = set(bigrammiPOS2)                        ProbCond1 = ProbCond(POS1, diversi1, bigrammiPOS1) #calcolo la probabilità condizionata sui bigrammi di POS        ProbCond2 = ProbCond(POS2, diversi2, bigrammiPOS2)        print("\nProbabilità condizionata dei bigrammi di POS del", file1, ":", ProbCond1) #stampo la probabilità condizionata        print("\nProbabilità condizionata dei bigrammi di POS del", file2, ":", ProbCond2)        ContaPOS1 = len(POS1) #con len conto gli elementi POS        ContaPOS2 = len(POS2)                LMutual1 = LocalMutual(POS1, ContaPOS1, bigrammiPOS1) #calcolo la LMI sui bigrammi di POS        LMutual2 = LocalMutual(POS2, ContaPOS2, bigrammiPOS2)        print("\nLMI dei bigrammi di POS del", file1, ":", LMutual1) #stampo la Local Mutual Information         print("\nLMI dei bigrammi di POS del", file2, ":", LMutual2)        main(sys.argv[1], sys.argv[2])